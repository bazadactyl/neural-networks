{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "**COMP 4107 Fall 2017**\n",
    "\n",
    "**Basim Ramadhan 100 901 646**\n",
    "\n",
    "**Christian Abbott 100 863 049**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Code\n",
    "\n",
    "We have provided an easy-to-use Makefile to help you run our program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make prepare-venv\n",
    "./env/bin/python hopfieldnet.py 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you have matplotlib, numpy, and scikit-learn installed already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "python3 hopfieldnet.py num_training_patterns\n",
    "# Example\n",
    "python3 hopfieldnet.py 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the program will do the following:\n",
    "\n",
    "1. load the MNIST dataset using scikit-learn\n",
    "2. pick some random MNIST images to train with; quantity of images is user-defined using the **num_training_patterns** command-line parameter\n",
    "3. initialize a Hopfield network\n",
    "4. train the network using Storkey's learning rule\n",
    "5. degrade each training image with 20% noise\n",
    "6. test whether the network can restore the degraded images satisfactorily\n",
    "7. print out the network's recovery accuracy\n",
    "\n",
    "After the programs performs the above, it will display the following visualizations:\n",
    "\n",
    "1. the network's weights\n",
    "2. the network's state (the sum of each neuron's own weights)\n",
    "3. a comparison between each original image, its degraded version, and its recovered version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Pattern Recovery / Classification\n",
    "\n",
    "We experimented with the Hebb and Storkey learning rules for Hopfield networks storing between 1 and 20 images. For each number of images stored in the network, we repeated the experiment 20 times. In other words, our experimentation was the following:\n",
    "\n",
    "* Train on 1 image with Hebb's rule and test the network's pattern recovery accuracy, 20 times.\n",
    "* Train on 2 images with Hebb's rule and test the network's pattern recovery accuracy, 20 times.\n",
    "* $\\cdots$\n",
    "* Train on 20 images with Hebb's rule and test the network's pattern recovery accuracy, 20 times.\n",
    "\n",
    "* Train on 1 image with Storkey's rule and test the network's pattern recovery accuracy, 20 times.\n",
    "* Train on 2 images with Storkey's rule and test the network's pattern recovery accuracy, 20 times.\n",
    "* $\\cdots$\n",
    "* Train on 20 images with Storkey's rule and test the network's pattern recovery accuracy, 20 times.\n",
    "\n",
    "A few notes regarding our experiments:\n",
    "\n",
    "* to degrade our images, we applied 20%, which means that a random 20% of the patterns bits were flipped\n",
    "* the threshold for a recovery being considered a success was the follow: the L2-norm between the original MNIST image and the recovered image must be less than 10. We found this threshold only lets very good recoveries pass\n",
    "\n",
    "These experiments yields the accuracy values in the code and chart below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Chart\n",
    "\n",
    "![accuracy.png](figures/q1/accuracy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "hebb_accuracy = [\n",
    "    100.00, 100.00, 16.67, 42.50, 11.00,\n",
    "    16.67, 7.14, 11.88, 7.78, 18.00,\n",
    "    10.45, 7.50, 5.77, 6.07, 7.00,\n",
    "    6.25, 8.24, 9.44, 3.95, 6.75\n",
    "]\n",
    "storkey_accuracy = [\n",
    "    100.00, 92.50, 68.33, 51.25, 44.00,\n",
    "    37.50, 34.29, 32.50, 27.78, 25.00,\n",
    "    24.55, 25.00, 23.08, 20.71, 21.00,\n",
    "    21.56, 17.65, 14.44, 16.58, 15.75\n",
    "]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "plt.plot(hebb_accuracy)\n",
    "plt.plot(storkey_accuracy)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel(\"Number of patterns stored in the network\")\n",
    "plt.ylabel(\"Accuracy of Degraded Pattern Recovery (%)\")\n",
    "plt.legend(['Hebb Learning Rule', 'Storkey Learning Rule'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Based on the data above---in addition to our experiences---we have the following conclusions:\n",
    "\n",
    "* In general, training with Storkey's rule produces more accurate networks.\n",
    "* Hebb's rule surprisingly works better for 2-image networks than Storkey's rule.\n",
    "* Training with Hebb's rule (<1 second) is significantly faster than with Storkey's rule (~5 seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Learning Rule Implementations\n",
    "\n",
    "For both learning rules, our initial implementations were simple and followed their respective definitions closely. These implementations were slow because they used nested for-loops with multiplications in each iteration. We then implemented optimized versions of the learning rules which used matrix operations instead. In the following code blocks we show both unoptimized and optimized implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hebb's Learning Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hebbian(self, patterns):\n",
    "    \"\"\"Train the Hopfield network using the Hebbian learning rule (1949).\n",
    "    https://en.wikipedia.org/wiki/Hopfield_network#Hebbian_learning_rule_for_Hopfield_networks\n",
    "    \"\"\"\n",
    "    for p in patterns:\n",
    "        a = p.reshape((self.shape, 1))\n",
    "        b = a.T\n",
    "        self.weights += np.dot(a, b)\n",
    "    self.weights -= (np.identity(patterns[0].size) * patterns.shape[0])\n",
    "    return self.weights\n",
    "\n",
    "def train_hebbian_unoptimized(self, patterns):\n",
    "    \"\"\"Inefficient version of the train_hebbian function.\n",
    "    Performs individual multiplications instead of efficient matrix operations.\"\"\"\n",
    "    n = self.shape\n",
    "    for i, j in itertools.product(range(n), range(n)):\n",
    "        self.weights[i][j] = sum([p[i] * p[j] for p in patterns]) / n\n",
    "    return self.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storkey's Learning Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_storkey(self, patterns):\n",
    "    \"\"\"Train the Hopfield network using the Storkey learning rule (1997).\n",
    "    https://en.wikipedia.org/wiki/Hopfield_network#The_Storkey_learning_rule\n",
    "    \"\"\"\n",
    "    n = self.shape\n",
    "    for p in patterns:\n",
    "        for i, j in itertools.product(range(n), range(n)):\n",
    "            wt = self.weights\n",
    "            w = wt[i][j]\n",
    "            x = p[i] * p[j]\n",
    "            y = p[i] * (np.dot(wt[j], p) - wt[j][i] * p[i] - wt[j][j] * p[j])\n",
    "            z = p[j] * (np.dot(wt[i], p) - wt[i][i] * p[i] - wt[i][j] * p[j])\n",
    "            wt[i][j] = w + ((x - y - z) / n)\n",
    "\n",
    "def train_storkey_unoptimized(self, patterns):\n",
    "    \"\"\"Inefficient version of the train_storkey function.\n",
    "    Performs individual multiplications instead of efficient matrix operations.\"\"\"\n",
    "    n = self.shape\n",
    "    for p in patterns:\n",
    "        for i, j in itertools.product(range(n), range(n)):\n",
    "            w = self.weights[i][j]\n",
    "            x = p[i] * p[j] / n\n",
    "            y = p[i] * sum([self.weights[j][k] * p[k] for k in range(n) if k not in [i, j]]) / n\n",
    "            z = p[j] * sum([self.weights[i][k] * p[k] for k in range(n) if k not in [i, j]]) / n\n",
    "            self.weights[i][j] = w + x - y - z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "Our activation function follows the definition on the Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(self, i):\n",
    "    \"\"\"Determine whether the given neuron should be active or inactive.\n",
    "    https://en.wikipedia.org/wiki/Hopfield_network#Updating\"\"\"\n",
    "    weight_sum = np.dot(self.weights[i], self.state)\n",
    "    self.state[i] = 1 if weight_sum > self.thresholds[i] else -1\n",
    "\n",
    "def activate_unoptimized(self, i):\n",
    "    \"\"\"Inefficient version of activate.\"\"\"\n",
    "    num_neurons = self.shape\n",
    "    weight_sum = 0.0\n",
    "    for j in range(num_neurons):\n",
    "        weight_sum += self.weights[i][j] * self.state[j]\n",
    "    self.state[i] = 1 if weight_sum > self.thresholds[i] else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovery\n",
    "\n",
    "We took our own approach to recovering degraded patterns. At each iteration, we call activate on each neuron in random order. If no state changes take place during a single iteration, then the network state is stable the image has (hopefully) been recovered. If the network is not stable yet, we repeat for another iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore(self, degraded_pattern):\n",
    "    \"\"\"Recover the original pattern of the degraded input pattern.\"\"\"\n",
    "    self.state = np.copy(degraded_pattern)\n",
    "    num_neurons = self.shape\n",
    "\n",
    "    # During each iteration: ensure each neuron is activated at least once\n",
    "    iterations = 0\n",
    "    while iterations < 10:\n",
    "        changed = False\n",
    "        neurons = list(range(num_neurons))\n",
    "        random.shuffle(neurons)\n",
    "        while neurons:\n",
    "            neuron = neurons.pop()\n",
    "            old_state = self.state[neuron]\n",
    "            self.activate(neuron)\n",
    "            new_state = self.state[neuron]\n",
    "            changed = True if old_state != new_state else changed\n",
    "        iterations += 1\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    recovered_pattern = np.copy(self.state)\n",
    "    return recovered_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Recovery Visualizations\n",
    "\n",
    "### Recovery from Hebb-trained Networks\n",
    "\n",
    "#### Success with 2-image Network\n",
    "\n",
    "![hebb-recovery-01.png](figures/q1/hebb-recovery-01.png)\n",
    "![hebb-recovery-02.png](figures/q1/hebb-recovery-02.png)\n",
    "\n",
    "#### Failure with 9-image Network\n",
    "\n",
    "![hebb-recovery-03.png](figures/q1/hebb-recovery-03.png)\n",
    "![hebb-recovery-04.png](figures/q1/hebb-recovery-04.png)\n",
    "\n",
    "### Recovery from Storkey-trained Networks\n",
    "\n",
    "#### Success with 2-image Network\n",
    "\n",
    "![storkey-recovery-01.png](figures/q1/storkey-recovery-01.png)\n",
    "![storkey-recovery-02.png](figures/q1/storkey-recovery-02.png)\n",
    "\n",
    "#### Success with 9-image Network\n",
    "\n",
    "![storkey-recovery-03.png](figures/q1/storkey-recovery-03.png)\n",
    "![storkey-recovery-04.png](figures/q1/storkey-recovery-04.png)\n",
    "\n",
    "#### Failure with 9-image Network\n",
    "\n",
    "![storkey-recovery-05.png](figures/q1/storkey-recovery-05.png)\n",
    "![storkey-recovery-06.png](figures/q1/storkey-recovery-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Visualizations\n",
    "\n",
    "We visualized the Hopfield networks after being trained. We did this to get a better understanding of how the and Hebb and Storkey learning rules differ. We found that Hebb's rule produced a simpler, less-detailed network.\n",
    "\n",
    "You can see this in the diagrams that follow: in Hebb-trained networks, the visualizations use fewer colors; this means that there's less information in the network. Meanwhile in Storkey-trained networks, the presence of more shades of color indicate a nuanced, more detailed network.\n",
    "\n",
    "Furthermore, it's also interesting to visually see patterns stored in the network in the \"Network State\" visualizations. We did this visualization by summing the weights of each neuron to decide on the color; then plotting a 28x28 grid of neurons with their respective colors.\n",
    "\n",
    "In both examples, two patterns were used to train the networks.\n",
    "\n",
    "### Hebb-trained Network\n",
    "\n",
    "![hebb-network-weights.png](figures/q1/hebb-network-weights.png)\n",
    "![hebb-network-state.png](figures/q1/hebb-network-state.png)\n",
    "\n",
    "### Storkey-trained Network\n",
    "\n",
    "![storkey-network-weights.png](figures/q1/storkey-network-weights.png)\n",
    "![storkey-network-state.png](figures/q1/storkey-network-state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Code\n",
    "\n",
    "We have provided an easy-to-use Makefile to help you run our program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make prepare-venv\n",
    "./env/bin/python som.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you have tensorflow, matplotlib, numpy, and scikit-learn installed already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 som.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the program will do the following:\n",
    "1. fetch MNIST data and retrieve the 1's and 5's; this will be our dataset\n",
    "2. initialize the SOM and train it on the entire dataset, in random order\n",
    "3. test the clustering accuracy of the trained SOM and print out the value\n",
    "4. perform K-means clustering on the dataset reduced to 2 dimensions\n",
    "5. test the clustering accuracy of the K-means clustering and print out the value\n",
    "\n",
    "After the programs performs the above, it will display three visualizations:\n",
    "1. the state of the SOM upon initialization\n",
    "2. the state of the SOM after training\n",
    "3. the K-means clustering of of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Clustering\n",
    "\n",
    "When you run our program it will print out the clustering accuracies of our trained SOM as well as our K-means output. We got the following accuracy values:\n",
    "* **SOM** clustering accuracy: **94.80%**\n",
    "* **K-means** clustering accuracy: **90.94%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM Dimensions\n",
    "\n",
    "In our implementation of an SOM, we used the following architecture:\n",
    "\n",
    "* **Input layer:** 784 neurons / features\n",
    "* **Output layer:** 900 neurons\n",
    "\n",
    "In our implementation, we found it useful to consider the output layer as a 30x30 grid of neurons. Note that this is identical to saying we used 900 output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM Parameters\n",
    "\n",
    "We used the following parameters for our SOM implementation:\n",
    "\n",
    "* Learning rate: 0.5\n",
    "* sigma ($\\sigma$): 5.0 (for the Gaussian that updates the winner neuron's neighbours)\n",
    "* Number of input neurons: 784\n",
    "* Number of output neurons: 900\n",
    "\n",
    "For our weights, we initialized them according to a normal distribution with $\\mu = 0.5$ and $\\sigma = 1.0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### SOM Network\n",
    "\n",
    "#### Before Training\n",
    "![som-network-state-before-training.png](figures/q2/som-network-state-before-training.png)\n",
    "\n",
    "#### After Training\n",
    "![som-network-state-after-training.png](figures/q2/som-network-state-after-training.png)\n",
    "\n",
    "### SOM Training Process\n",
    "\n",
    "We were interested in visualizing the SOM's training process. We created visualizations after each training iteration to see how the SOM changes over time. We start off in the top left with random weights. We progressivly train with MNIST images until, at iteration 16 (bottom right) we have a clear partitioning of the map into two partitions. Training any more just makes the map dance around more, not improving the map very much.\n",
    "\n",
    "The purple partition represents MNIST images of the number 1. The yellow partition represent MNIST images of the number 5.\n",
    "\n",
    "![som-training-00.png](figures/q2/som-training-00.png)\n",
    "![som-training-01.png](figures/q2/som-training-01.png)\n",
    "![som-training-02.png](figures/q2/som-training-02.png)\n",
    "![som-training-03.png](figures/q2/som-training-03.png)\n",
    "![som-training-04.png](figures/q2/som-training-04.png)\n",
    "![som-training-05.png](figures/q2/som-training-05.png)\n",
    "![som-training-06.png](figures/q2/som-training-06.png)\n",
    "![som-training-07.png](figures/q2/som-training-07.png)\n",
    "![som-training-08.png](figures/q2/som-training-08.png)\n",
    "![som-training-09.png](figures/q2/som-training-09.png)\n",
    "![som-training-10.png](figures/q2/som-training-10.png)\n",
    "![som-training-11.png](figures/q2/som-training-11.png)\n",
    "![som-training-12.png](figures/q2/som-training-12.png)\n",
    "![som-training-13.png](figures/q2/som-training-13.png)\n",
    "![som-training-14.png](figures/q2/som-training-14.png)\n",
    "![som-training-15.png](figures/q2/som-training-15.png)\n",
    "\n",
    "### SOM Output Neuron Prototypes\n",
    "\n",
    "We were interested in visualizing a few neurons' prototypes. A single neuron's weights to the input later (784 weights in our case) represents its prototype. The following charts visualize the prototypes of a few randomly selected prototypes. Seeing these neat visualizations help us be confident that our SOM is behaving correctly.\n",
    "\n",
    "![neuron-prototype-01.png](figures/q2/neuron-prototype-01.png)\n",
    "![neuron-prototype-01.png](figures/q2/neuron-prototype-02.png)\n",
    "![neuron-prototype-01.png](figures/q2/neuron-prototype-03.png)\n",
    "![neuron-prototype-01.png](figures/q2/neuron-prototype-04.png)\n",
    "![neuron-prototype-01.png](figures/q2/neuron-prototype-05.png)\n",
    "![neuron-prototype-01.png](figures/q2/neuron-prototype-06.png)\n",
    "\n",
    "\n",
    "### K-means Clustering\n",
    "\n",
    "We wanted to keep the comparison between SOM \"clustering\" and K-means clustering close, so we let $k = 2$. After performing the K-means clustering, we were able to tell if each element was correctly clustered because we know the label for each MNIST image. This allowed us to create the chart below, which indicated the elements that were clustered correctly and those that were not. We also show the cluster centroids computed by the K-means algorithm.\n",
    "\n",
    "![kmeans-clustering-small-dots.png](figures/q2/kmeans-clustering-small-dots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Code\n",
    "\n",
    "**Important:** our submission includes the LFW face data that our program needs. We had to create a separate Python 2.7 utility obtain this data because the scikit-learn tutorial requires PIL (an outdated, little-supported) to read the data. We had to compile PIL from source, so it's easier for us to include the data instead of making tou compile PIL and run yet another script.\n",
    "\n",
    "We have provided an easy-to-use Makefile to help you run our program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make prepare-venv\n",
    "./env/bin/python eigenfaces.py      # don't perform PCA on face data\n",
    "./env/bin/python eigenfaces.py 100  # perform PCA to obtain 100 components / eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, if you have tensorflow, matplotlib, numpy, and scikit-learn installed already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "python3 eigenfaces.py [num_PCA_components]\n",
    "# Examplee\n",
    "python3 eigenfaces.py      # don't perform PCA on face data\n",
    "python3 eigenfaces.py 100  # perform PCA to obtain 100 eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the program will do the following:\n",
    "\n",
    "1. import the LFW face data from local files (provided with our submission)\n",
    "2. perform 10-fold cross-validation; for each fold do:\n",
    "    * initialize a feed-forward network from scratch\n",
    "    * train the classifier on all 9 training folds for 100 epochs\n",
    "    * after the final epoch, test for accuracy using the testing fold\n",
    "    * save the accuracy for this fold\n",
    "3. compute the average accuracy over the 10 folds and print it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Classification / Facial Recognition\n",
    "\n",
    "First, we tested our feed-forward network with the face data to see how well we can classify faces without the use of PCA. We performed 10-fold cross validation, along with the following hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_folds = 10\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.020\n",
    "\n",
    "# Architecture (USED FOR ALL EXPERIMENTS)\n",
    "input_neurons = 1850    # face images were 50x37 in size\n",
    "hidden_neurons_1 = 160  # hidden layer 1\n",
    "hidden_neurons_2 = 60   # hidden layer 2\n",
    "output_neurons = 7      # dataset had faces of 7 unique people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded a classifier with 83.39% averaged cross-validated accuracy.\n",
    "\n",
    "We then applied PCA to the data before training on it, again using 10-fold cross-validation. We tried several values for the number of PCA components (eigenfaces) to reduce the data to. **We used the exact same architecture and hyper-parameters as the experiment on the raw face data.** These experiments on the dimensionality-reduced datasets yielded the cross-validated accuracy values in the chart below:\n",
    "\n",
    "![pca-accuracy.png](figures/eigenfaces/pca-accuracy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "non_pca_accuracy = 83.39\n",
    "\n",
    "num_eigenfaces = [\n",
    "    100, 60, 40, 20,\n",
    "     14, 12, 10,  9,\n",
    "      8,  7,  6,  5,\n",
    "      4,  3,  2,  1,\n",
    "]\n",
    "pca_accuracy = [\n",
    "    82.37, 80.36, 76.40, 70.34,\n",
    "    62.50, 53.19, 51.32, 48.99,\n",
    "    48.99, 48.06, 47.13, 46.74,\n",
    "    44.10, 41.15, 41.15, 41.15,\n",
    "]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "plt.axhline(y=non_pca_accuracy, color='red')\n",
    "plt.plot(num_eigenfaces, pca_accuracy)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.axis([100, 0, 30, 100])\n",
    "plt.xlabel(\"Number of PCA Components\")\n",
    "plt.ylabel(\"10-fold Cross-Validated Facial Recognition Accuracy (%)\")\n",
    "plt.legend(['Data without PCA', 'Data with X-many PCA components'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing PCA\n",
    "\n",
    "We used scikit-learn to perform PCA on the input face data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = int(sys.argv[1])\n",
    "pca = PCA(n_components=num_components, svd_solver='randomized', whiten=True).fit(data)\n",
    "data = pca.transform(data)\n",
    "num_features = data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross-validation\n",
    "\n",
    "In our training process, our program performs the following:\n",
    "* for each of the 10 folds:\n",
    "    * initialize a feed-forward network from scratch\n",
    "    * train the classifier on all training folds for 100 epochs\n",
    "    * after the final epoch, save the accuracy using the testing fold\n",
    "* compute the average accuracy over the 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(num_folds):\n",
    "    print('Using fold {:02d} / {:02d} as the training fold:'.format(fold + 1, num_folds))\n",
    "\n",
    "    train_indices, test_indices = folds[fold]\n",
    "    trX, teX = data[train_indices], data[test_indices]\n",
    "    trY, teY = labels[train_indices], labels[test_indices]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            batch_starts = range(0, len(trX), batch_size)\n",
    "            batch_ends = range(batch_size, len(trX) + 1, batch_size)\n",
    "\n",
    "            for start, end in zip(batch_starts, batch_ends):\n",
    "                sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                epoch_accuracy = np.mean(np.argmax(teY, axis=1) == sess.run(predict_op, feed_dict={X: teX}))\n",
    "                print('\\tEpoch {:3} ---> {:.2f}%'.format(epoch, epoch_accuracy * 100))\n",
    "\n",
    "    fold_accuracy.append(epoch_accuracy)\n",
    "    print('Accuracy with fold #{} as training: {:.2f}%\\n'.format(fold + 1, fold_accuracy[-1] * 100))\n",
    "\n",
    "accuracy = sum(fold_accuracy) / len(fold_accuracy)\n",
    "print('Average {}-fold cross validation accuracy: {:.2f}%'.format(num_folds, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## Question 1\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Hopfield_network\n",
    "* http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.103&rep=rep1&type=pdf (Storkey's paper)\n",
    "\n",
    "## Question 2\n",
    "\n",
    "* https://github.com/JustGlowing/minisom\n",
    "* https://en.wikipedia.org/wiki/Self-organizing_map\n",
    "\n",
    "## Question 3\n",
    "\n",
    "* We used the TensorFlow feed-forward code from our in-class tutorials.\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "* http://scikit-learn.org/0.18/auto_examples/applications/face_recognition.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
